JL-DCF Structure
JL_DCF(
  (JLModule): JLModule(
    (backbone): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (relu): ReLU(inplace=True)
    (vgg_conv1): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
    )
    (CP): ModuleList(
      (0): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
        (4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
        (4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (1): ReLU(inplace=True)
        (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (3): ReLU(inplace=True)
        (4): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): Conv2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (1): ReLU(inplace=True)
        (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (3): ReLU(inplace=True)
        (4): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
      (4): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (1): ReLU(inplace=True)
        (2): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (3): ReLU(inplace=True)
        (4): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
      (5): Sequential(
        (0): Conv2d(2048, 512, kernel_size=(7, 7), stride=(1, 1), padding=(6, 6), dilation=(2, 2))
        (1): ReLU(inplace=True)
        (2): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(6, 6), dilation=(2, 2))
        (3): ReLU(inplace=True)
        (4): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
    )
  )
  (FA): ModuleList(
    (0): FAModule(
      (relu): ReLU(inplace=True)
      (conv_branch1): Sequential(
        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
      )
      (conv_branch2): Sequential(
        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
      )
      (conv_branch3): Sequential(
        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
        (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
      (conv_branch4): Sequential(
        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
        (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (2): ReLU(inplace=True)
      )
    )
    (1): FAModule(
      (relu): ReLU(inplace=True)
      (conv_branch1): Sequential(
        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
      )
      (conv_branch2): Sequential(
        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
      )
      (conv_branch3): Sequential(
        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
        (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
      (conv_branch4): Sequential(
        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
        (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (2): ReLU(inplace=True)
      )
    )
    (2): FAModule(
      (relu): ReLU(inplace=True)
      (conv_branch1): Sequential(
        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
      )
      (conv_branch2): Sequential(
        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
      )
      (conv_branch3): Sequential(
        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
        (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
      (conv_branch4): Sequential(
        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
        (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (2): ReLU(inplace=True)
      )
    )
    (3): FAModule(
      (relu): ReLU(inplace=True)
      (conv_branch1): Sequential(
        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
      )
      (conv_branch2): Sequential(
        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
      )
      (conv_branch3): Sequential(
        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
        (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
      (conv_branch4): Sequential(
        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
        (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (2): ReLU(inplace=True)
      )
    )
    (4): FAModule(
      (relu): ReLU(inplace=True)
      (conv_branch1): Sequential(
        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
      )
      (conv_branch2): Sequential(
        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
      )
      (conv_branch3): Sequential(
        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
        (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU(inplace=True)
      )
      (conv_branch4): Sequential(
        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
        (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (2): ReLU(inplace=True)
      )
    )
  )
  (upsampling): ModuleList(
    (0): ModuleList(
      (0): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
    (1): ModuleList(
      (0): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): ConvTranspose2d(64, 64, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
    )
    (2): ModuleList(
      (0): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): ConvTranspose2d(64, 64, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): ConvTranspose2d(64, 64, kernel_size=(16, 16), stride=(8, 8), padding=(4, 4))
    )
    (3): ModuleList(
      (0): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): ConvTranspose2d(64, 64, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): ConvTranspose2d(64, 64, kernel_size=(16, 16), stride=(8, 8), padding=(4, 4))
      (3): ConvTranspose2d(64, 64, kernel_size=(32, 32), stride=(16, 16), padding=(8, 8))
    )
  )
  (score_JL): ScoreLayer(
    (score): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
  )
  (score_DCF): ScoreLayer(
    (score): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
  )
  (cm): CMLayer(
    (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
    (sigmoid): Sigmoid()
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
    (channel): ChannelAttention(
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (relu1): ReLU()
      (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (sigmoid): Sigmoid()
    )
    (spatial): SpatialAttention(
      (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (sigmoid): Sigmoid()
    )
    (aspp): _ASPP(
      (c0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (c1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))
      (c2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5))
      (c3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7))
    )
  )
)
The number of parameters: 143857675

----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 320, 320]           1,792
              ReLU-2         [-1, 64, 320, 320]               0
            Conv2d-3         [-1, 64, 320, 320]          36,928
              ReLU-4         [-1, 64, 320, 320]               0
            Conv2d-5        [-1, 128, 320, 320]          73,856
              ReLU-6        [-1, 128, 320, 320]               0
              ReLU-7        [-1, 128, 320, 320]               0
              ReLU-8        [-1, 128, 320, 320]               0
              ReLU-9        [-1, 128, 320, 320]               0
             ReLU-10        [-1, 128, 320, 320]               0
             ReLU-11        [-1, 128, 320, 320]               0
             ReLU-12        [-1, 128, 320, 320]               0
           Conv2d-13        [-1, 128, 320, 320]         147,584
             ReLU-14        [-1, 128, 320, 320]               0
             ReLU-15        [-1, 128, 320, 320]               0
             ReLU-16        [-1, 128, 320, 320]               0
             ReLU-17        [-1, 128, 320, 320]               0
             ReLU-18        [-1, 128, 320, 320]               0
             ReLU-19        [-1, 128, 320, 320]               0
             ReLU-20        [-1, 128, 320, 320]               0
           Conv2d-21         [-1, 64, 320, 320]          73,792
             ReLU-22         [-1, 64, 320, 320]               0
             ReLU-23         [-1, 64, 320, 320]               0
             ReLU-24         [-1, 64, 320, 320]               0
             ReLU-25         [-1, 64, 320, 320]               0
             ReLU-26         [-1, 64, 320, 320]               0
             ReLU-27         [-1, 64, 320, 320]               0
             ReLU-28         [-1, 64, 320, 320]               0
           Conv2d-29         [-1, 64, 160, 160]           9,408
      BatchNorm2d-30         [-1, 64, 160, 160]             128
             ReLU-31         [-1, 64, 160, 160]               0
        MaxPool2d-32           [-1, 64, 80, 80]               0
           Conv2d-33           [-1, 64, 80, 80]           4,096
      BatchNorm2d-34           [-1, 64, 80, 80]             128
             ReLU-35           [-1, 64, 80, 80]               0
           Conv2d-36           [-1, 64, 80, 80]          36,864
      BatchNorm2d-37           [-1, 64, 80, 80]             128
             ReLU-38           [-1, 64, 80, 80]               0
           Conv2d-39          [-1, 256, 80, 80]          16,384
      BatchNorm2d-40          [-1, 256, 80, 80]             512
           Conv2d-41          [-1, 256, 80, 80]          16,384
      BatchNorm2d-42          [-1, 256, 80, 80]             512
             ReLU-43          [-1, 256, 80, 80]               0
       Bottleneck-44          [-1, 256, 80, 80]               0
           Conv2d-45           [-1, 64, 80, 80]          16,384
      BatchNorm2d-46           [-1, 64, 80, 80]             128
             ReLU-47           [-1, 64, 80, 80]               0
           Conv2d-48           [-1, 64, 80, 80]          36,864
      BatchNorm2d-49           [-1, 64, 80, 80]             128
             ReLU-50           [-1, 64, 80, 80]               0
           Conv2d-51          [-1, 256, 80, 80]          16,384
      BatchNorm2d-52          [-1, 256, 80, 80]             512
             ReLU-53          [-1, 256, 80, 80]               0
       Bottleneck-54          [-1, 256, 80, 80]               0
           Conv2d-55           [-1, 64, 80, 80]          16,384
      BatchNorm2d-56           [-1, 64, 80, 80]             128
             ReLU-57           [-1, 64, 80, 80]               0
           Conv2d-58           [-1, 64, 80, 80]          36,864
      BatchNorm2d-59           [-1, 64, 80, 80]             128
             ReLU-60           [-1, 64, 80, 80]               0
           Conv2d-61          [-1, 256, 80, 80]          16,384
      BatchNorm2d-62          [-1, 256, 80, 80]             512
             ReLU-63          [-1, 256, 80, 80]               0
       Bottleneck-64          [-1, 256, 80, 80]               0
           Conv2d-65          [-1, 128, 40, 40]          32,768
      BatchNorm2d-66          [-1, 128, 40, 40]             256
             ReLU-67          [-1, 128, 40, 40]               0
           Conv2d-68          [-1, 128, 40, 40]         147,456
      BatchNorm2d-69          [-1, 128, 40, 40]             256
             ReLU-70          [-1, 128, 40, 40]               0
           Conv2d-71          [-1, 512, 40, 40]          65,536
      BatchNorm2d-72          [-1, 512, 40, 40]           1,024
           Conv2d-73          [-1, 512, 40, 40]         131,072
      BatchNorm2d-74          [-1, 512, 40, 40]           1,024
             ReLU-75          [-1, 512, 40, 40]               0
       Bottleneck-76          [-1, 512, 40, 40]               0
           Conv2d-77          [-1, 128, 40, 40]          65,536
      BatchNorm2d-78          [-1, 128, 40, 40]             256
             ReLU-79          [-1, 128, 40, 40]               0
           Conv2d-80          [-1, 128, 40, 40]         147,456
      BatchNorm2d-81          [-1, 128, 40, 40]             256
             ReLU-82          [-1, 128, 40, 40]               0
           Conv2d-83          [-1, 512, 40, 40]          65,536
      BatchNorm2d-84          [-1, 512, 40, 40]           1,024
             ReLU-85          [-1, 512, 40, 40]               0
       Bottleneck-86          [-1, 512, 40, 40]               0
           Conv2d-87          [-1, 128, 40, 40]          65,536
      BatchNorm2d-88          [-1, 128, 40, 40]             256
             ReLU-89          [-1, 128, 40, 40]               0
           Conv2d-90          [-1, 128, 40, 40]         147,456
      BatchNorm2d-91          [-1, 128, 40, 40]             256
             ReLU-92          [-1, 128, 40, 40]               0
           Conv2d-93          [-1, 512, 40, 40]          65,536
      BatchNorm2d-94          [-1, 512, 40, 40]           1,024
             ReLU-95          [-1, 512, 40, 40]               0
       Bottleneck-96          [-1, 512, 40, 40]               0
           Conv2d-97          [-1, 128, 40, 40]          65,536
      BatchNorm2d-98          [-1, 128, 40, 40]             256
             ReLU-99          [-1, 128, 40, 40]               0
          Conv2d-100          [-1, 128, 40, 40]         147,456
     BatchNorm2d-101          [-1, 128, 40, 40]             256
            ReLU-102          [-1, 128, 40, 40]               0
          Conv2d-103          [-1, 512, 40, 40]          65,536
     BatchNorm2d-104          [-1, 512, 40, 40]           1,024
            ReLU-105          [-1, 512, 40, 40]               0
      Bottleneck-106          [-1, 512, 40, 40]               0
          Conv2d-107          [-1, 256, 20, 20]         131,072
     BatchNorm2d-108          [-1, 256, 20, 20]             512
            ReLU-109          [-1, 256, 20, 20]               0
          Conv2d-110          [-1, 256, 20, 20]         589,824
     BatchNorm2d-111          [-1, 256, 20, 20]             512
            ReLU-112          [-1, 256, 20, 20]               0
          Conv2d-113         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-114         [-1, 1024, 20, 20]           2,048
          Conv2d-115         [-1, 1024, 20, 20]         524,288
     BatchNorm2d-116         [-1, 1024, 20, 20]           2,048
            ReLU-117         [-1, 1024, 20, 20]               0
      Bottleneck-118         [-1, 1024, 20, 20]               0
          Conv2d-119          [-1, 256, 20, 20]         262,144
     BatchNorm2d-120          [-1, 256, 20, 20]             512
            ReLU-121          [-1, 256, 20, 20]               0
          Conv2d-122          [-1, 256, 20, 20]         589,824
     BatchNorm2d-123          [-1, 256, 20, 20]             512
            ReLU-124          [-1, 256, 20, 20]               0
          Conv2d-125         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-126         [-1, 1024, 20, 20]           2,048
            ReLU-127         [-1, 1024, 20, 20]               0
      Bottleneck-128         [-1, 1024, 20, 20]               0
          Conv2d-129          [-1, 256, 20, 20]         262,144
     BatchNorm2d-130          [-1, 256, 20, 20]             512
            ReLU-131          [-1, 256, 20, 20]               0
          Conv2d-132          [-1, 256, 20, 20]         589,824
     BatchNorm2d-133          [-1, 256, 20, 20]             512
            ReLU-134          [-1, 256, 20, 20]               0
          Conv2d-135         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-136         [-1, 1024, 20, 20]           2,048
            ReLU-137         [-1, 1024, 20, 20]               0
      Bottleneck-138         [-1, 1024, 20, 20]               0
          Conv2d-139          [-1, 256, 20, 20]         262,144
     BatchNorm2d-140          [-1, 256, 20, 20]             512
            ReLU-141          [-1, 256, 20, 20]               0
          Conv2d-142          [-1, 256, 20, 20]         589,824
     BatchNorm2d-143          [-1, 256, 20, 20]             512
            ReLU-144          [-1, 256, 20, 20]               0
          Conv2d-145         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-146         [-1, 1024, 20, 20]           2,048
            ReLU-147         [-1, 1024, 20, 20]               0
      Bottleneck-148         [-1, 1024, 20, 20]               0
          Conv2d-149          [-1, 256, 20, 20]         262,144
     BatchNorm2d-150          [-1, 256, 20, 20]             512
            ReLU-151          [-1, 256, 20, 20]               0
          Conv2d-152          [-1, 256, 20, 20]         589,824
     BatchNorm2d-153          [-1, 256, 20, 20]             512
            ReLU-154          [-1, 256, 20, 20]               0
          Conv2d-155         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-156         [-1, 1024, 20, 20]           2,048
            ReLU-157         [-1, 1024, 20, 20]               0
      Bottleneck-158         [-1, 1024, 20, 20]               0
          Conv2d-159          [-1, 256, 20, 20]         262,144
     BatchNorm2d-160          [-1, 256, 20, 20]             512
            ReLU-161          [-1, 256, 20, 20]               0
          Conv2d-162          [-1, 256, 20, 20]         589,824
     BatchNorm2d-163          [-1, 256, 20, 20]             512
            ReLU-164          [-1, 256, 20, 20]               0
          Conv2d-165         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-166         [-1, 1024, 20, 20]           2,048
            ReLU-167         [-1, 1024, 20, 20]               0
      Bottleneck-168         [-1, 1024, 20, 20]               0
          Conv2d-169          [-1, 256, 20, 20]         262,144
     BatchNorm2d-170          [-1, 256, 20, 20]             512
            ReLU-171          [-1, 256, 20, 20]               0
          Conv2d-172          [-1, 256, 20, 20]         589,824
     BatchNorm2d-173          [-1, 256, 20, 20]             512
            ReLU-174          [-1, 256, 20, 20]               0
          Conv2d-175         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-176         [-1, 1024, 20, 20]           2,048
            ReLU-177         [-1, 1024, 20, 20]               0
      Bottleneck-178         [-1, 1024, 20, 20]               0
          Conv2d-179          [-1, 256, 20, 20]         262,144
     BatchNorm2d-180          [-1, 256, 20, 20]             512
            ReLU-181          [-1, 256, 20, 20]               0
          Conv2d-182          [-1, 256, 20, 20]         589,824
     BatchNorm2d-183          [-1, 256, 20, 20]             512
            ReLU-184          [-1, 256, 20, 20]               0
          Conv2d-185         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-186         [-1, 1024, 20, 20]           2,048
            ReLU-187         [-1, 1024, 20, 20]               0
      Bottleneck-188         [-1, 1024, 20, 20]               0
          Conv2d-189          [-1, 256, 20, 20]         262,144
     BatchNorm2d-190          [-1, 256, 20, 20]             512
            ReLU-191          [-1, 256, 20, 20]               0
          Conv2d-192          [-1, 256, 20, 20]         589,824
     BatchNorm2d-193          [-1, 256, 20, 20]             512
            ReLU-194          [-1, 256, 20, 20]               0
          Conv2d-195         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-196         [-1, 1024, 20, 20]           2,048
            ReLU-197         [-1, 1024, 20, 20]               0
      Bottleneck-198         [-1, 1024, 20, 20]               0
          Conv2d-199          [-1, 256, 20, 20]         262,144
     BatchNorm2d-200          [-1, 256, 20, 20]             512
            ReLU-201          [-1, 256, 20, 20]               0
          Conv2d-202          [-1, 256, 20, 20]         589,824
     BatchNorm2d-203          [-1, 256, 20, 20]             512
            ReLU-204          [-1, 256, 20, 20]               0
          Conv2d-205         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-206         [-1, 1024, 20, 20]           2,048
            ReLU-207         [-1, 1024, 20, 20]               0
      Bottleneck-208         [-1, 1024, 20, 20]               0
          Conv2d-209          [-1, 256, 20, 20]         262,144
     BatchNorm2d-210          [-1, 256, 20, 20]             512
            ReLU-211          [-1, 256, 20, 20]               0
          Conv2d-212          [-1, 256, 20, 20]         589,824
     BatchNorm2d-213          [-1, 256, 20, 20]             512
            ReLU-214          [-1, 256, 20, 20]               0
          Conv2d-215         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-216         [-1, 1024, 20, 20]           2,048
            ReLU-217         [-1, 1024, 20, 20]               0
      Bottleneck-218         [-1, 1024, 20, 20]               0
          Conv2d-219          [-1, 256, 20, 20]         262,144
     BatchNorm2d-220          [-1, 256, 20, 20]             512
            ReLU-221          [-1, 256, 20, 20]               0
          Conv2d-222          [-1, 256, 20, 20]         589,824
     BatchNorm2d-223          [-1, 256, 20, 20]             512
            ReLU-224          [-1, 256, 20, 20]               0
          Conv2d-225         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-226         [-1, 1024, 20, 20]           2,048
            ReLU-227         [-1, 1024, 20, 20]               0
      Bottleneck-228         [-1, 1024, 20, 20]               0
          Conv2d-229          [-1, 256, 20, 20]         262,144
     BatchNorm2d-230          [-1, 256, 20, 20]             512
            ReLU-231          [-1, 256, 20, 20]               0
          Conv2d-232          [-1, 256, 20, 20]         589,824
     BatchNorm2d-233          [-1, 256, 20, 20]             512
            ReLU-234          [-1, 256, 20, 20]               0
          Conv2d-235         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-236         [-1, 1024, 20, 20]           2,048
            ReLU-237         [-1, 1024, 20, 20]               0
      Bottleneck-238         [-1, 1024, 20, 20]               0
          Conv2d-239          [-1, 256, 20, 20]         262,144
     BatchNorm2d-240          [-1, 256, 20, 20]             512
            ReLU-241          [-1, 256, 20, 20]               0
          Conv2d-242          [-1, 256, 20, 20]         589,824
     BatchNorm2d-243          [-1, 256, 20, 20]             512
            ReLU-244          [-1, 256, 20, 20]               0
          Conv2d-245         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-246         [-1, 1024, 20, 20]           2,048
            ReLU-247         [-1, 1024, 20, 20]               0
      Bottleneck-248         [-1, 1024, 20, 20]               0
          Conv2d-249          [-1, 256, 20, 20]         262,144
     BatchNorm2d-250          [-1, 256, 20, 20]             512
            ReLU-251          [-1, 256, 20, 20]               0
          Conv2d-252          [-1, 256, 20, 20]         589,824
     BatchNorm2d-253          [-1, 256, 20, 20]             512
            ReLU-254          [-1, 256, 20, 20]               0
          Conv2d-255         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-256         [-1, 1024, 20, 20]           2,048
            ReLU-257         [-1, 1024, 20, 20]               0
      Bottleneck-258         [-1, 1024, 20, 20]               0
          Conv2d-259          [-1, 256, 20, 20]         262,144
     BatchNorm2d-260          [-1, 256, 20, 20]             512
            ReLU-261          [-1, 256, 20, 20]               0
          Conv2d-262          [-1, 256, 20, 20]         589,824
     BatchNorm2d-263          [-1, 256, 20, 20]             512
            ReLU-264          [-1, 256, 20, 20]               0
          Conv2d-265         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-266         [-1, 1024, 20, 20]           2,048
            ReLU-267         [-1, 1024, 20, 20]               0
      Bottleneck-268         [-1, 1024, 20, 20]               0
          Conv2d-269          [-1, 256, 20, 20]         262,144
     BatchNorm2d-270          [-1, 256, 20, 20]             512
            ReLU-271          [-1, 256, 20, 20]               0
          Conv2d-272          [-1, 256, 20, 20]         589,824
     BatchNorm2d-273          [-1, 256, 20, 20]             512
            ReLU-274          [-1, 256, 20, 20]               0
          Conv2d-275         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-276         [-1, 1024, 20, 20]           2,048
            ReLU-277         [-1, 1024, 20, 20]               0
      Bottleneck-278         [-1, 1024, 20, 20]               0
          Conv2d-279          [-1, 256, 20, 20]         262,144
     BatchNorm2d-280          [-1, 256, 20, 20]             512
            ReLU-281          [-1, 256, 20, 20]               0
          Conv2d-282          [-1, 256, 20, 20]         589,824
     BatchNorm2d-283          [-1, 256, 20, 20]             512
            ReLU-284          [-1, 256, 20, 20]               0
          Conv2d-285         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-286         [-1, 1024, 20, 20]           2,048
            ReLU-287         [-1, 1024, 20, 20]               0
      Bottleneck-288         [-1, 1024, 20, 20]               0
          Conv2d-289          [-1, 256, 20, 20]         262,144
     BatchNorm2d-290          [-1, 256, 20, 20]             512
            ReLU-291          [-1, 256, 20, 20]               0
          Conv2d-292          [-1, 256, 20, 20]         589,824
     BatchNorm2d-293          [-1, 256, 20, 20]             512
            ReLU-294          [-1, 256, 20, 20]               0
          Conv2d-295         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-296         [-1, 1024, 20, 20]           2,048
            ReLU-297         [-1, 1024, 20, 20]               0
      Bottleneck-298         [-1, 1024, 20, 20]               0
          Conv2d-299          [-1, 256, 20, 20]         262,144
     BatchNorm2d-300          [-1, 256, 20, 20]             512
            ReLU-301          [-1, 256, 20, 20]               0
          Conv2d-302          [-1, 256, 20, 20]         589,824
     BatchNorm2d-303          [-1, 256, 20, 20]             512
            ReLU-304          [-1, 256, 20, 20]               0
          Conv2d-305         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-306         [-1, 1024, 20, 20]           2,048
            ReLU-307         [-1, 1024, 20, 20]               0
      Bottleneck-308         [-1, 1024, 20, 20]               0
          Conv2d-309          [-1, 256, 20, 20]         262,144
     BatchNorm2d-310          [-1, 256, 20, 20]             512
            ReLU-311          [-1, 256, 20, 20]               0
          Conv2d-312          [-1, 256, 20, 20]         589,824
     BatchNorm2d-313          [-1, 256, 20, 20]             512
            ReLU-314          [-1, 256, 20, 20]               0
          Conv2d-315         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-316         [-1, 1024, 20, 20]           2,048
            ReLU-317         [-1, 1024, 20, 20]               0
      Bottleneck-318         [-1, 1024, 20, 20]               0
          Conv2d-319          [-1, 256, 20, 20]         262,144
     BatchNorm2d-320          [-1, 256, 20, 20]             512
            ReLU-321          [-1, 256, 20, 20]               0
          Conv2d-322          [-1, 256, 20, 20]         589,824
     BatchNorm2d-323          [-1, 256, 20, 20]             512
            ReLU-324          [-1, 256, 20, 20]               0
          Conv2d-325         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-326         [-1, 1024, 20, 20]           2,048
            ReLU-327         [-1, 1024, 20, 20]               0
      Bottleneck-328         [-1, 1024, 20, 20]               0
          Conv2d-329          [-1, 256, 20, 20]         262,144
     BatchNorm2d-330          [-1, 256, 20, 20]             512
            ReLU-331          [-1, 256, 20, 20]               0
          Conv2d-332          [-1, 256, 20, 20]         589,824
     BatchNorm2d-333          [-1, 256, 20, 20]             512
            ReLU-334          [-1, 256, 20, 20]               0
          Conv2d-335         [-1, 1024, 20, 20]         262,144
     BatchNorm2d-336         [-1, 1024, 20, 20]           2,048
            ReLU-337         [-1, 1024, 20, 20]               0
      Bottleneck-338         [-1, 1024, 20, 20]               0
          Conv2d-339          [-1, 512, 20, 20]         524,288
     BatchNorm2d-340          [-1, 512, 20, 20]           1,024
            ReLU-341          [-1, 512, 20, 20]               0
          Conv2d-342          [-1, 512, 20, 20]       2,359,296
     BatchNorm2d-343          [-1, 512, 20, 20]           1,024
            ReLU-344          [-1, 512, 20, 20]               0
          Conv2d-345         [-1, 2048, 20, 20]       1,048,576
     BatchNorm2d-346         [-1, 2048, 20, 20]           4,096
          Conv2d-347         [-1, 2048, 20, 20]       2,097,152
     BatchNorm2d-348         [-1, 2048, 20, 20]           4,096
            ReLU-349         [-1, 2048, 20, 20]               0
      Bottleneck-350         [-1, 2048, 20, 20]               0
          Conv2d-351          [-1, 512, 20, 20]       1,048,576
     BatchNorm2d-352          [-1, 512, 20, 20]           1,024
            ReLU-353          [-1, 512, 20, 20]               0
          Conv2d-354          [-1, 512, 20, 20]       2,359,296
     BatchNorm2d-355          [-1, 512, 20, 20]           1,024
            ReLU-356          [-1, 512, 20, 20]               0
          Conv2d-357         [-1, 2048, 20, 20]       1,048,576
     BatchNorm2d-358         [-1, 2048, 20, 20]           4,096
            ReLU-359         [-1, 2048, 20, 20]               0
      Bottleneck-360         [-1, 2048, 20, 20]               0
          Conv2d-361          [-1, 512, 20, 20]       1,048,576
     BatchNorm2d-362          [-1, 512, 20, 20]           1,024
            ReLU-363          [-1, 512, 20, 20]               0
          Conv2d-364          [-1, 512, 20, 20]       2,359,296
     BatchNorm2d-365          [-1, 512, 20, 20]           1,024
            ReLU-366          [-1, 512, 20, 20]               0
          Conv2d-367         [-1, 2048, 20, 20]       1,048,576
     BatchNorm2d-368         [-1, 2048, 20, 20]           4,096
            ReLU-369         [-1, 2048, 20, 20]               0
      Bottleneck-370         [-1, 2048, 20, 20]               0
          ResNet-371  [[-1, 64, 160, 160], [-1, 256, 80, 80], [-1, 512, 40, 40], [-1, 1024, 20, 20], [-1, 2048, 20, 20]]               0
          Conv2d-372        [-1, 128, 160, 160]          73,856
            ReLU-373        [-1, 128, 160, 160]               0
            ReLU-374        [-1, 128, 160, 160]               0
            ReLU-375        [-1, 128, 160, 160]               0
            ReLU-376        [-1, 128, 160, 160]               0
            ReLU-377        [-1, 128, 160, 160]               0
            ReLU-378        [-1, 128, 160, 160]               0
            ReLU-379        [-1, 128, 160, 160]               0
          Conv2d-380        [-1, 128, 160, 160]         147,584
            ReLU-381        [-1, 128, 160, 160]               0
            ReLU-382        [-1, 128, 160, 160]               0
            ReLU-383        [-1, 128, 160, 160]               0
            ReLU-384        [-1, 128, 160, 160]               0
            ReLU-385        [-1, 128, 160, 160]               0
            ReLU-386        [-1, 128, 160, 160]               0
            ReLU-387        [-1, 128, 160, 160]               0
          Conv2d-388         [-1, 64, 160, 160]          73,792
            ReLU-389         [-1, 64, 160, 160]               0
            ReLU-390         [-1, 64, 160, 160]               0
            ReLU-391         [-1, 64, 160, 160]               0
            ReLU-392         [-1, 64, 160, 160]               0
            ReLU-393         [-1, 64, 160, 160]               0
            ReLU-394         [-1, 64, 160, 160]               0
            ReLU-395         [-1, 64, 160, 160]               0
          Conv2d-396          [-1, 256, 80, 80]       1,638,656
            ReLU-397          [-1, 256, 80, 80]               0
            ReLU-398          [-1, 256, 80, 80]               0
            ReLU-399          [-1, 256, 80, 80]               0
            ReLU-400          [-1, 256, 80, 80]               0
            ReLU-401          [-1, 256, 80, 80]               0
            ReLU-402          [-1, 256, 80, 80]               0
            ReLU-403          [-1, 256, 80, 80]               0
          Conv2d-404          [-1, 256, 80, 80]       1,638,656
            ReLU-405          [-1, 256, 80, 80]               0
            ReLU-406          [-1, 256, 80, 80]               0
            ReLU-407          [-1, 256, 80, 80]               0
            ReLU-408          [-1, 256, 80, 80]               0
            ReLU-409          [-1, 256, 80, 80]               0
            ReLU-410          [-1, 256, 80, 80]               0
            ReLU-411          [-1, 256, 80, 80]               0
          Conv2d-412           [-1, 64, 80, 80]         147,520
            ReLU-413           [-1, 64, 80, 80]               0
            ReLU-414           [-1, 64, 80, 80]               0
            ReLU-415           [-1, 64, 80, 80]               0
            ReLU-416           [-1, 64, 80, 80]               0
            ReLU-417           [-1, 64, 80, 80]               0
            ReLU-418           [-1, 64, 80, 80]               0
            ReLU-419           [-1, 64, 80, 80]               0
          Conv2d-420          [-1, 256, 40, 40]       3,277,056
            ReLU-421          [-1, 256, 40, 40]               0
            ReLU-422          [-1, 256, 40, 40]               0
            ReLU-423          [-1, 256, 40, 40]               0
            ReLU-424          [-1, 256, 40, 40]               0
            ReLU-425          [-1, 256, 40, 40]               0
            ReLU-426          [-1, 256, 40, 40]               0
            ReLU-427          [-1, 256, 40, 40]               0
          Conv2d-428          [-1, 256, 40, 40]       1,638,656
            ReLU-429          [-1, 256, 40, 40]               0
            ReLU-430          [-1, 256, 40, 40]               0
            ReLU-431          [-1, 256, 40, 40]               0
            ReLU-432          [-1, 256, 40, 40]               0
            ReLU-433          [-1, 256, 40, 40]               0
            ReLU-434          [-1, 256, 40, 40]               0
            ReLU-435          [-1, 256, 40, 40]               0
          Conv2d-436           [-1, 64, 40, 40]         147,520
            ReLU-437           [-1, 64, 40, 40]               0
            ReLU-438           [-1, 64, 40, 40]               0
            ReLU-439           [-1, 64, 40, 40]               0
            ReLU-440           [-1, 64, 40, 40]               0
            ReLU-441           [-1, 64, 40, 40]               0
            ReLU-442           [-1, 64, 40, 40]               0
            ReLU-443           [-1, 64, 40, 40]               0
          Conv2d-444          [-1, 512, 20, 20]      13,107,712
            ReLU-445          [-1, 512, 20, 20]               0
            ReLU-446          [-1, 512, 20, 20]               0
            ReLU-447          [-1, 512, 20, 20]               0
            ReLU-448          [-1, 512, 20, 20]               0
            ReLU-449          [-1, 512, 20, 20]               0
            ReLU-450          [-1, 512, 20, 20]               0
            ReLU-451          [-1, 512, 20, 20]               0
          Conv2d-452          [-1, 512, 20, 20]       6,554,112
            ReLU-453          [-1, 512, 20, 20]               0
            ReLU-454          [-1, 512, 20, 20]               0
            ReLU-455          [-1, 512, 20, 20]               0
            ReLU-456          [-1, 512, 20, 20]               0
            ReLU-457          [-1, 512, 20, 20]               0
            ReLU-458          [-1, 512, 20, 20]               0
            ReLU-459          [-1, 512, 20, 20]               0
          Conv2d-460           [-1, 64, 20, 20]         294,976
            ReLU-461           [-1, 64, 20, 20]               0
            ReLU-462           [-1, 64, 20, 20]               0
            ReLU-463           [-1, 64, 20, 20]               0
            ReLU-464           [-1, 64, 20, 20]               0
            ReLU-465           [-1, 64, 20, 20]               0
            ReLU-466           [-1, 64, 20, 20]               0
            ReLU-467           [-1, 64, 20, 20]               0
          Conv2d-468          [-1, 512, 20, 20]      51,380,736
            ReLU-469          [-1, 512, 20, 20]               0
            ReLU-470          [-1, 512, 20, 20]               0
            ReLU-471          [-1, 512, 20, 20]               0
            ReLU-472          [-1, 512, 20, 20]               0
            ReLU-473          [-1, 512, 20, 20]               0
            ReLU-474          [-1, 512, 20, 20]               0
            ReLU-475          [-1, 512, 20, 20]               0
          Conv2d-476          [-1, 512, 20, 20]      12,845,568
            ReLU-477          [-1, 512, 20, 20]               0
            ReLU-478          [-1, 512, 20, 20]               0
            ReLU-479          [-1, 512, 20, 20]               0
            ReLU-480          [-1, 512, 20, 20]               0
            ReLU-481          [-1, 512, 20, 20]               0
            ReLU-482          [-1, 512, 20, 20]               0
            ReLU-483          [-1, 512, 20, 20]               0
          Conv2d-484           [-1, 64, 20, 20]         294,976
            ReLU-485           [-1, 64, 20, 20]               0
            ReLU-486           [-1, 64, 20, 20]               0
            ReLU-487           [-1, 64, 20, 20]               0
            ReLU-488           [-1, 64, 20, 20]               0
            ReLU-489           [-1, 64, 20, 20]               0
            ReLU-490           [-1, 64, 20, 20]               0
            ReLU-491           [-1, 64, 20, 20]               0
        JLModule-492  [[-1, 64, 320, 320], [-1, 64, 160, 160], [-1, 64, 80, 80], [-1, 64, 40, 40], [-1, 64, 20, 20], [-1, 64, 20, 20]]               0
         CMLayer-493  [[-1, 64, 320, 320], [-1, 64, 160, 160], [-1, 64, 80, 80], [-1, 64, 40, 40], [-1, 64, 20, 20], [-1, 64, 20, 20]]               0
          Conv2d-494            [-1, 1, 20, 20]              65
      ScoreLayer-495            [-1, 1, 20, 20]               0
          Conv2d-496           [-1, 16, 20, 20]           1,040
            ReLU-497           [-1, 16, 20, 20]               0
            ReLU-498           [-1, 16, 20, 20]               0
            ReLU-499           [-1, 16, 20, 20]               0
            ReLU-500           [-1, 16, 20, 20]               0
            ReLU-501           [-1, 16, 20, 20]               0
          Conv2d-502           [-1, 32, 20, 20]           2,080
            ReLU-503           [-1, 32, 20, 20]               0
            ReLU-504           [-1, 32, 20, 20]               0
            ReLU-505           [-1, 32, 20, 20]               0
            ReLU-506           [-1, 32, 20, 20]               0
            ReLU-507           [-1, 32, 20, 20]               0
          Conv2d-508           [-1, 16, 20, 20]           4,624
            ReLU-509           [-1, 16, 20, 20]               0
            ReLU-510           [-1, 16, 20, 20]               0
            ReLU-511           [-1, 16, 20, 20]               0
            ReLU-512           [-1, 16, 20, 20]               0
            ReLU-513           [-1, 16, 20, 20]               0
          Conv2d-514           [-1, 16, 20, 20]           1,040
            ReLU-515           [-1, 16, 20, 20]               0
            ReLU-516           [-1, 16, 20, 20]               0
            ReLU-517           [-1, 16, 20, 20]               0
            ReLU-518           [-1, 16, 20, 20]               0
            ReLU-519           [-1, 16, 20, 20]               0
          Conv2d-520           [-1, 16, 20, 20]           6,416
            ReLU-521           [-1, 16, 20, 20]               0
            ReLU-522           [-1, 16, 20, 20]               0
            ReLU-523           [-1, 16, 20, 20]               0
            ReLU-524           [-1, 16, 20, 20]               0
            ReLU-525           [-1, 16, 20, 20]               0
       MaxPool2d-526           [-1, 64, 20, 20]               0
          Conv2d-527           [-1, 16, 20, 20]           1,040
            ReLU-528           [-1, 16, 20, 20]               0
            ReLU-529           [-1, 16, 20, 20]               0
            ReLU-530           [-1, 16, 20, 20]               0
            ReLU-531           [-1, 16, 20, 20]               0
            ReLU-532           [-1, 16, 20, 20]               0
        FAModule-533           [-1, 64, 20, 20]               0
 ConvTranspose2d-534           [-1, 64, 40, 40]          65,600
          Conv2d-535           [-1, 16, 40, 40]           1,040
            ReLU-536           [-1, 16, 40, 40]               0
            ReLU-537           [-1, 16, 40, 40]               0
            ReLU-538           [-1, 16, 40, 40]               0
            ReLU-539           [-1, 16, 40, 40]               0
            ReLU-540           [-1, 16, 40, 40]               0
          Conv2d-541           [-1, 32, 40, 40]           2,080
            ReLU-542           [-1, 32, 40, 40]               0
            ReLU-543           [-1, 32, 40, 40]               0
            ReLU-544           [-1, 32, 40, 40]               0
            ReLU-545           [-1, 32, 40, 40]               0
            ReLU-546           [-1, 32, 40, 40]               0
          Conv2d-547           [-1, 16, 40, 40]           4,624
            ReLU-548           [-1, 16, 40, 40]               0
            ReLU-549           [-1, 16, 40, 40]               0
            ReLU-550           [-1, 16, 40, 40]               0
            ReLU-551           [-1, 16, 40, 40]               0
            ReLU-552           [-1, 16, 40, 40]               0
          Conv2d-553           [-1, 16, 40, 40]           1,040
            ReLU-554           [-1, 16, 40, 40]               0
            ReLU-555           [-1, 16, 40, 40]               0
            ReLU-556           [-1, 16, 40, 40]               0
            ReLU-557           [-1, 16, 40, 40]               0
            ReLU-558           [-1, 16, 40, 40]               0
          Conv2d-559           [-1, 16, 40, 40]           6,416
            ReLU-560           [-1, 16, 40, 40]               0
            ReLU-561           [-1, 16, 40, 40]               0
            ReLU-562           [-1, 16, 40, 40]               0
            ReLU-563           [-1, 16, 40, 40]               0
            ReLU-564           [-1, 16, 40, 40]               0
       MaxPool2d-565           [-1, 64, 40, 40]               0
          Conv2d-566           [-1, 16, 40, 40]           1,040
            ReLU-567           [-1, 16, 40, 40]               0
            ReLU-568           [-1, 16, 40, 40]               0
            ReLU-569           [-1, 16, 40, 40]               0
            ReLU-570           [-1, 16, 40, 40]               0
            ReLU-571           [-1, 16, 40, 40]               0
        FAModule-572           [-1, 64, 40, 40]               0
 ConvTranspose2d-573           [-1, 64, 80, 80]         262,208
 ConvTranspose2d-574           [-1, 64, 80, 80]          65,600
          Conv2d-575           [-1, 16, 80, 80]           1,040
            ReLU-576           [-1, 16, 80, 80]               0
            ReLU-577           [-1, 16, 80, 80]               0
            ReLU-578           [-1, 16, 80, 80]               0
            ReLU-579           [-1, 16, 80, 80]               0
            ReLU-580           [-1, 16, 80, 80]               0
          Conv2d-581           [-1, 32, 80, 80]           2,080
            ReLU-582           [-1, 32, 80, 80]               0
            ReLU-583           [-1, 32, 80, 80]               0
            ReLU-584           [-1, 32, 80, 80]               0
            ReLU-585           [-1, 32, 80, 80]               0
            ReLU-586           [-1, 32, 80, 80]               0
          Conv2d-587           [-1, 16, 80, 80]           4,624
            ReLU-588           [-1, 16, 80, 80]               0
            ReLU-589           [-1, 16, 80, 80]               0
            ReLU-590           [-1, 16, 80, 80]               0
            ReLU-591           [-1, 16, 80, 80]               0
            ReLU-592           [-1, 16, 80, 80]               0
          Conv2d-593           [-1, 16, 80, 80]           1,040
            ReLU-594           [-1, 16, 80, 80]               0
            ReLU-595           [-1, 16, 80, 80]               0
            ReLU-596           [-1, 16, 80, 80]               0
            ReLU-597           [-1, 16, 80, 80]               0
            ReLU-598           [-1, 16, 80, 80]               0
          Conv2d-599           [-1, 16, 80, 80]           6,416
            ReLU-600           [-1, 16, 80, 80]               0
            ReLU-601           [-1, 16, 80, 80]               0
            ReLU-602           [-1, 16, 80, 80]               0
            ReLU-603           [-1, 16, 80, 80]               0
            ReLU-604           [-1, 16, 80, 80]               0
       MaxPool2d-605           [-1, 64, 80, 80]               0
          Conv2d-606           [-1, 16, 80, 80]           1,040
            ReLU-607           [-1, 16, 80, 80]               0
            ReLU-608           [-1, 16, 80, 80]               0
            ReLU-609           [-1, 16, 80, 80]               0
            ReLU-610           [-1, 16, 80, 80]               0
            ReLU-611           [-1, 16, 80, 80]               0
        FAModule-612           [-1, 64, 80, 80]               0
 ConvTranspose2d-613         [-1, 64, 160, 160]       1,048,640
 ConvTranspose2d-614         [-1, 64, 160, 160]         262,208
 ConvTranspose2d-615         [-1, 64, 160, 160]          65,600
          Conv2d-616         [-1, 16, 160, 160]           1,040
            ReLU-617         [-1, 16, 160, 160]               0
            ReLU-618         [-1, 16, 160, 160]               0
            ReLU-619         [-1, 16, 160, 160]               0
            ReLU-620         [-1, 16, 160, 160]               0
            ReLU-621         [-1, 16, 160, 160]               0
          Conv2d-622         [-1, 32, 160, 160]           2,080
            ReLU-623         [-1, 32, 160, 160]               0
            ReLU-624         [-1, 32, 160, 160]               0
            ReLU-625         [-1, 32, 160, 160]               0
            ReLU-626         [-1, 32, 160, 160]               0
            ReLU-627         [-1, 32, 160, 160]               0
          Conv2d-628         [-1, 16, 160, 160]           4,624
            ReLU-629         [-1, 16, 160, 160]               0
            ReLU-630         [-1, 16, 160, 160]               0
            ReLU-631         [-1, 16, 160, 160]               0
            ReLU-632         [-1, 16, 160, 160]               0
            ReLU-633         [-1, 16, 160, 160]               0
          Conv2d-634         [-1, 16, 160, 160]           1,040
            ReLU-635         [-1, 16, 160, 160]               0
            ReLU-636         [-1, 16, 160, 160]               0
            ReLU-637         [-1, 16, 160, 160]               0
            ReLU-638         [-1, 16, 160, 160]               0
            ReLU-639         [-1, 16, 160, 160]               0
          Conv2d-640         [-1, 16, 160, 160]           6,416
            ReLU-641         [-1, 16, 160, 160]               0
            ReLU-642         [-1, 16, 160, 160]               0
            ReLU-643         [-1, 16, 160, 160]               0
            ReLU-644         [-1, 16, 160, 160]               0
            ReLU-645         [-1, 16, 160, 160]               0
       MaxPool2d-646         [-1, 64, 160, 160]               0
          Conv2d-647         [-1, 16, 160, 160]           1,040
            ReLU-648         [-1, 16, 160, 160]               0
            ReLU-649         [-1, 16, 160, 160]               0
            ReLU-650         [-1, 16, 160, 160]               0
            ReLU-651         [-1, 16, 160, 160]               0
            ReLU-652         [-1, 16, 160, 160]               0
        FAModule-653         [-1, 64, 160, 160]               0
 ConvTranspose2d-654         [-1, 64, 320, 320]       4,194,368
 ConvTranspose2d-655         [-1, 64, 320, 320]       1,048,640
 ConvTranspose2d-656         [-1, 64, 320, 320]         262,208
 ConvTranspose2d-657         [-1, 64, 320, 320]          65,600
          Conv2d-658         [-1, 16, 320, 320]           1,040
            ReLU-659         [-1, 16, 320, 320]               0
            ReLU-660         [-1, 16, 320, 320]               0
            ReLU-661         [-1, 16, 320, 320]               0
            ReLU-662         [-1, 16, 320, 320]               0
            ReLU-663         [-1, 16, 320, 320]               0
          Conv2d-664         [-1, 32, 320, 320]           2,080
            ReLU-665         [-1, 32, 320, 320]               0
            ReLU-666         [-1, 32, 320, 320]               0
            ReLU-667         [-1, 32, 320, 320]               0
            ReLU-668         [-1, 32, 320, 320]               0
            ReLU-669         [-1, 32, 320, 320]               0
          Conv2d-670         [-1, 16, 320, 320]           4,624
            ReLU-671         [-1, 16, 320, 320]               0
            ReLU-672         [-1, 16, 320, 320]               0
            ReLU-673         [-1, 16, 320, 320]               0
            ReLU-674         [-1, 16, 320, 320]               0
            ReLU-675         [-1, 16, 320, 320]               0
          Conv2d-676         [-1, 16, 320, 320]           1,040
            ReLU-677         [-1, 16, 320, 320]               0
            ReLU-678         [-1, 16, 320, 320]               0
            ReLU-679         [-1, 16, 320, 320]               0
            ReLU-680         [-1, 16, 320, 320]               0
            ReLU-681         [-1, 16, 320, 320]               0
          Conv2d-682         [-1, 16, 320, 320]           6,416
            ReLU-683         [-1, 16, 320, 320]               0
            ReLU-684         [-1, 16, 320, 320]               0
            ReLU-685         [-1, 16, 320, 320]               0
            ReLU-686         [-1, 16, 320, 320]               0
            ReLU-687         [-1, 16, 320, 320]               0
       MaxPool2d-688         [-1, 64, 320, 320]               0
          Conv2d-689         [-1, 16, 320, 320]           1,040
            ReLU-690         [-1, 16, 320, 320]               0
            ReLU-691         [-1, 16, 320, 320]               0
            ReLU-692         [-1, 16, 320, 320]               0
            ReLU-693         [-1, 16, 320, 320]               0
            ReLU-694         [-1, 16, 320, 320]               0
        FAModule-695         [-1, 64, 320, 320]               0
          Conv2d-696          [-1, 1, 320, 320]              65
      ScoreLayer-697          [-1, 1, 320, 320]               0
================================================================
Total params: 143,517,490
Trainable params: 143,517,490
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 1.17
Forward/backward pass size (MB): 5052.06
Params size (MB): 547.48
Estimated Total Size (MB): 5600.70
----------------------------------------------------------------